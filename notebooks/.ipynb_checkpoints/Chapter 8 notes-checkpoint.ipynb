{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction\n",
    "\n",
    "Many features make both training slow and expands the search space making solutions harder to find for machine learning algorithims - curse of dimensionality.\n",
    "\n",
    "Features are often correlated or are unimportant, so can be removed without impacting results on the task. If the feature space can be reduced it can make models less complex and pipelines simpler to run.\n",
    "\n",
    "Some features in a dataset can be constant or highly correlated, if this is true then the data are close to a lower dimensional representation.\n",
    "\n",
    "## Projection approach\n",
    "\n",
    "As observations are not uniform, we can find a lower dimensional sub-space where the data can be represented. for example; there may exist a 2d plane in a 3d space that can represent the features whilst conserving most of the varience of the data.\n",
    "\n",
    "## manifold approach\n",
    "\n",
    "If the representation 'twists' in subspace, projecting onto a lower dimensional representation won't work. IN this case, we need to 'unfold' the data onto a lower dimensional space. Often this 'manifold' assumption is emperically observed. \n",
    "\n",
    "The main aim of both these approaches is to make the problem, and therefore the solution simpler. Whether this is possible or not depends on the problem itself. The only real constant benifit is training the data takes less time with fewer features.\n",
    "\n",
    "# PCA\n",
    "Finds the hyperplane closest to the data, then projects the data onto it. But how do we find it?\n",
    "The best hyperplane is one that preserves the maximum variance of the data or that it reduces the mean squared distance between the original dataset and the new projection.\n",
    "\n",
    "PCA then finds the next axis to project onto, this is orthagonal to the first. This can be repeated for as many features as there are in the dataset. \n",
    "\n",
    "Each hyperplane is a principle component, so the $i^{th}$ axis to project onto is the $i^{th}$ principle component.\n",
    "We choose i number of axes to form our hyperplane\n",
    "\n",
    "Principle components is not stable, so if the training data is peturbed then the PCs will be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=100, n_features=10, noise=20, bias=50, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "always centre the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_centred = X-X.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, Vt = np.linalg.svd(X_centred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc1 = Vt.T[:,0]\n",
    "pc2 = Vt.T[:,1]\n",
    "# etc...\n",
    "hyperplane = Vt.T[:,:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explained variance ratio\n",
    "The explained varience out of the total variance in the dataset for each individual component/axis.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_projects-PtpjDyvs",
   "language": "python",
   "name": "ml_projects-ptpjdyvs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
